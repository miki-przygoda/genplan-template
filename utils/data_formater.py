"""
Combined data formatting utility for the FloorPlans970 dataset.

This module provides a complete workflow:
1. Download datasets from Hugging Face if needed
2. Load the dataset from disk or Hugging Face Hub
3. Extract room polygons and create masks
4. Save processed data to the processed directory
"""

from __future__ import annotations

import argparse
import json
import os
import random
import re
import time
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

import cv2 as cv
import numpy as np
from datasets import Dataset, DatasetDict, load_dataset, load_from_disk

try:
    from PIL import Image
    PILImage = Image  # Alias for consistency
except ImportError:  # pragma: no cover - pillow is listed in requirements
    Image = None  # type: ignore
    PILImage = None  # type: ignore

# Constants
DEFAULT_DATASET_NAME = "HamzaWajid1/FloorPlans970Dataset"
DEFAULT_LOCAL_DATASET_PATH = Path("dataset") / "FloorPlans970Dataset"
DEFAULT_PROCESSED_DIR = Path("processed")
DEFAULT_NO_TEXT_PATH = Path("processed") / "no_text_ids.json"
EXPECTED_IMAGE_SHAPE = (512, 512)

DATASETS = ["HamzaWajid1/FloorPlans970Dataset"]

PREFERRED_TEXT_COLUMNS: Sequence[str] = (
    "supporting_text",
    "supporting_texts",
    "description",
    "text",
)
PREFERRED_ID_COLUMNS: Sequence[str] = (
    "image_id",
    "plan_id",
    "id",
    "file_name",
    "filename",
)
PREFERRED_IMAGE_COLUMNS: Sequence[str] = ("image", "png", "jpg", "jpeg")


class DataInspectionError(RuntimeError):
    """Raised when an expected dataset field cannot be resolved."""


# ============================================================================
# Dataset Download Functions
# ============================================================================

def download_datasets():
    """
    Download datasets from Hugging Face and organize them into folders
    based on the dataset name (second part of the repository path).
    Includes rate limiting protection with random delays.
    """
    os.makedirs("dataset", exist_ok=True)
    
    for i, dataset_path in enumerate(DATASETS):
        dataset_name = dataset_path.split("/")[-1]
        dataset_folder = os.path.join("dataset", dataset_name)
        
        print(f"Downloading {dataset_path}...")
        print(f"Creating folder: {dataset_folder}")
        
        if i > 0:
            delay = random.uniform(2, 5)
            print(f"‚è≥ Waiting {delay:.1f} seconds to avoid rate limiting...")
            time.sleep(delay)
        
        try:
            os.makedirs(dataset_folder, exist_ok=True)
            
            print("   Loading dataset (this may take a while for large datasets)...")
            dataset = load_dataset(dataset_path, verification_mode="no_checks")
            
            dataset.save_to_disk(dataset_folder)
            
            print(f"‚úÖ Successfully downloaded and saved {dataset_path} to {dataset_folder}")
            print(f"   Dataset info: {dataset}")
            print("-" * 50)
            
        except Exception as e:
            print(f"‚ùå Error downloading {dataset_path}: {str(e)}")
            
            if "expected" in str(e) and "recorded" in str(e):
                print("   üí° This appears to be a dataset metadata mismatch error.")
                print("   üí° The dataset may have been updated but metadata is outdated.")
                print("   üí° Try downloading this dataset manually or contact the dataset owner.")
            elif "Connection" in str(e) or "timeout" in str(e).lower():
                print("   üí° This appears to be a network connectivity issue.")
                print("   üí° Check your internet connection and try again.")
            else:
                print("   üí° Unknown error - check the dataset URL and try again.")
            
            print("-" * 50)
            
            if i < len(DATASETS) - 1:
                error_delay = random.uniform(5, 10)
                print(f"‚è≥ Waiting {error_delay:.1f} seconds before next download...")
                time.sleep(error_delay)


# ============================================================================
# Dataset Loading Functions
# ============================================================================

def load_floorplans_dataset(
    dataset_path: Optional[Path] = None,
    *,
    dataset_name: str = DEFAULT_DATASET_NAME,
    split: Optional[str] = "train",
) -> Dataset:
    """
    Load the FloorPlans970 dataset from disk if available; otherwise fall back
    to Hugging Face Hub.
    """
    if dataset_path is None:
        dataset_path = DEFAULT_LOCAL_DATASET_PATH

    dataset: Dataset | DatasetDict
    try:
        dataset = load_from_disk(str(dataset_path))
    except Exception as e:
        print(f"Error loading dataset from disk: {e}")
        print("Falling back to Hugging Face Hub")
        dataset = load_dataset(dataset_name, verification_mode="no_checks")
        dataset.save_to_disk(str(dataset_path))

    if isinstance(dataset, DatasetDict):
        if split is not None and split in dataset:
            return dataset[split]
        if "train" in dataset:
            return dataset["train"]
        # Fall back to the first available split.
        first_split = next(iter(dataset.keys()))
        return dataset[first_split]

    return dataset


def resolve_column(column_names: Sequence[str], candidates: Sequence[str]) -> str:
    """Pick the first column name that exists within the dataset."""
    for candidate in candidates:
        if candidate in column_names:
            return candidate
    raise DataInspectionError(
        f"None of the candidate columns {candidates} were found in {column_names}."
    )


# ============================================================================
# Mask Extraction Functions
# ============================================================================

def extract_room_polygons(pil_image, min_area_ratio: float = 0.001, corner_quality: float = 0.01):
    """Return filled room mask, room contours, metadata, and snapped corners."""

    # Convert PIL image to OpenCV-friendly arrays
    rgb_image = np.array(pil_image.convert("RGB"))
    gray = cv.cvtColor(rgb_image, cv.COLOR_RGB2GRAY)

    # Enhance structural edges and connect potential gaps in walls
    blurred = cv.GaussianBlur(gray, (5, 5), 0)
    edges = cv.Canny(blurred, 40, 120)

    wall_kernel = cv.getStructuringElement(cv.MORPH_RECT, (3, 3))
    thick_walls = cv.dilate(edges, wall_kernel, iterations=2)
    thick_walls = cv.morphologyEx(thick_walls, cv.MORPH_CLOSE, wall_kernel, iterations=2)

    # Invert to isolate open space, then flood fill the exterior to remove background
    open_space = cv.bitwise_not(thick_walls)
    floodfilled = open_space.copy()
    height, width = open_space.shape
    flood_mask = np.zeros((height + 2, width + 2), dtype=np.uint8)
    cv.floodFill(floodfilled, flood_mask, (0, 0), 0)

    room_mask = cv.morphologyEx(floodfilled, cv.MORPH_OPEN, wall_kernel, iterations=2)

    contours, _ = cv.findContours(room_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)
    min_area = min_area_ratio * (height * width)

    filled_mask = np.zeros_like(room_mask)
    polygons = []
    metadata = []

    corner_candidates = cv.goodFeaturesToTrack(
        thick_walls,
        maxCorners=500,
        qualityLevel=corner_quality,
        minDistance=10,
        blockSize=7,
        useHarrisDetector=True,
        k=0.04,
    )

    deduplication_threshold = 6
    unique_corners = []
    if corner_candidates is not None:
        for candidate in corner_candidates:
            x, y = candidate.ravel()
            x, y = int(x), int(y)
            matched = False
            for stored in unique_corners:
                if abs(stored[0] - x) <= deduplication_threshold and abs(stored[1] - y) <= deduplication_threshold:
                    stored[0] = int((stored[0] * stored[2] + x) / (stored[2] + 1))
                    stored[1] = int((stored[1] * stored[2] + y) / (stored[2] + 1))
                    stored[2] += 1
                    matched = True
                    break
            if not matched:
                unique_corners.append([x, y, 1])

    corner_points = [(corner[0], corner[1]) for corner in unique_corners]
    for contour in contours:
        area = cv.contourArea(contour)
        if area < min_area:
            continue

        epsilon = 0.02 * cv.arcLength(contour, True)
        approx = cv.approxPolyDP(contour, epsilon, True)
        polygon = [(int(point[0][0]), int(point[0][1])) for point in approx]

        cv.drawContours(filled_mask, [contour], contourIdx=-1, color=255, thickness=cv.FILLED)

        moments = cv.moments(contour)
        if moments["m00"] != 0:
            centroid = (int(moments["m10"] / moments["m00"]), int(moments["m01"] / moments["m00"]))
        else:
            centroid = (0, 0)

        corner_ids = []
        if corner_points:
            for vertex in polygon:
                distances = [np.linalg.norm(np.array(vertex) - np.array(corner)) for corner in corner_points]
                nearest = int(np.argmin(distances))
                corner_ids.append(nearest)

        polygons.append(contour)
        metadata.append(
            {
                "room_id": len(metadata),
                "area_px": int(area),
                "polygon": polygon,
                "centroid": centroid,
                "corner_ids": corner_ids,
            }
        )

    return filled_mask, polygons, metadata, corner_points


def colourise_rooms(rgb_image: np.ndarray, room_polygons, corner_points):
    """Create an overlay and blended visualisation for segmented rooms."""

    overlay = np.zeros_like(rgb_image)
    rng = np.random.default_rng(seed=42)

    for index, contour in enumerate(room_polygons):
        colour = rng.integers(0, 255, size=3).tolist()
        cv.fillPoly(overlay, [contour], colour)

    for corner in corner_points:
        cv.circle(overlay, corner, radius=4, color=(255, 255, 255), thickness=-1)

    blended = cv.addWeighted(rgb_image, 0.6, overlay, 0.4, gamma=0)
    return overlay, blended


# ============================================================================
# Utility Functions
# ============================================================================

def _derive_record_identifier(record: Dict[str, Any], index: int) -> str:
    """Return a stable identifier for a dataset record."""
    for key in ("image_id", "plan_id", "id", "file_name", "filename"):
        value = record.get(key)
        if value is None:
            continue
        if isinstance(value, str) and value.strip():
            return value
        return str(value)
    return f"sample_{index:05d}"


def _normalise_identifier(identifier: str) -> str:
    """Sanitise identifier for safe file naming."""
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", identifier).strip("._-")
    return cleaned or "sample"


def _prepare_room_metadata(raw_metadata):
    """Convert metadata into JSON-serialisable form."""
    serialisable = []
    for room in raw_metadata:
        room_entry = {
            "room_id": int(room["room_id"]),
            "area_px": int(room["area_px"]),
            "polygon": [list(vertex) for vertex in room["polygon"]],
            "centroid": list(room["centroid"]),
            "corner_ids": [int(corner_id) for corner_id in room["corner_ids"]],
        }
        serialisable.append(room_entry)
    return serialisable


def write_json(path: Path, data: Any) -> None:
    """Persist a JSON payload to disk with pretty indentation."""
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2, ensure_ascii=False) + "\n")


# ============================================================================
# Main Workflow
# ============================================================================

def process_dataset(
    dataset_path: Optional[Path] = None,
    *,
    dataset_name: str = DEFAULT_DATASET_NAME,
    split: Optional[str] = "train",
    image_column: Optional[str] = None,
    output_dir: Optional[Path] = None,
    force_download: bool = False,
):
    """
    Complete workflow: download (if needed), load dataset, extract masks, and save to processed.
    
    Args:
        dataset_path: Path to locally saved dataset
        dataset_name: Hugging Face dataset identifier
        split: Dataset split to load
        image_column: Override image column name
        output_dir: Directory to save processed data
        force_download: Force re-download even if dataset exists locally
    """
    if output_dir is None:
        output_dir = DEFAULT_PROCESSED_DIR / "floor_plans"
    
    # Step 1: Download dataset if needed (only if missing, not forced unless specified)
    dataset_path_to_check = dataset_path or DEFAULT_LOCAL_DATASET_PATH
    if force_download or not dataset_path_to_check.exists():
        print("=" * 50)
        print("Step 1: Downloading dataset...")
        print("=" * 50)
        download_datasets()
        print()
    
    # Step 2: Load dataset
    print("=" * 50)
    print("Step 2: Loading dataset...")
    print("=" * 50)
    dataset = load_floorplans_dataset(
        dataset_path=dataset_path,
        dataset_name=dataset_name,
        split=split,
    )
    
    # Resolve image column
    try:
        resolved_image_column = image_column or resolve_column(
            dataset.column_names, PREFERRED_IMAGE_COLUMNS
        )
    except DataInspectionError:
        resolved_image_column = "image"
        print(f"Warning: Could not resolve image column, using '{resolved_image_column}'")
    
    # Resolve text column
    try:
        resolved_text_column = resolve_column(
            dataset.column_names, PREFERRED_TEXT_COLUMNS
        )
    except DataInspectionError:
        resolved_text_column = None
        print(f"Warning: Could not resolve text column, supporting text will not be included")
    
    # Step 2.5: Regenerate no_text_ids.json
    print("=" * 50)
    print("Step 2.5: Regenerating no_text_ids.json...")
    print("=" * 50)
    no_text_ids = set()
    if resolved_text_column:
        for idx, record in enumerate(dataset):
            text_value = record.get(resolved_text_column)
            is_missing = False
            
            if text_value is None:
                is_missing = True
            elif isinstance(text_value, str):
                is_missing = text_value.strip() == ""
            elif isinstance(text_value, (list, tuple, set)):
                is_missing = all((not item or str(item).strip() == "") for item in text_value)
            
            if is_missing:
                # Use 1-based ID to match floor numbering (floor001, floor002, etc.)
                floor_id = idx + 1
                no_text_ids.add(floor_id)
        
        # Save to processed/no_text_ids.json
        no_text_output_path = Path("processed") / "no_text_ids.json"
        no_text_output_path.parent.mkdir(parents=True, exist_ok=True)
        write_json(no_text_output_path, sorted(list(no_text_ids)))
        print(f"Regenerated no_text_ids.json with {len(no_text_ids)} records without text")
        print(f"Saved to: {no_text_output_path}")
    else:
        print("Skipping no_text_ids.json generation: text column not available")
    print()
    
    print(f"Loaded dataset with {len(dataset)} records")
    print(f"Using image column: '{resolved_image_column}'")
    if resolved_text_column:
        print(f"Using text column: '{resolved_text_column}'")
    print()
    
    # Step 3: Process images and create masks
    print("=" * 50)
    print("Step 3: Processing images and creating masks...")
    print("=" * 50)
    print("Note: All existing files will be overwritten.")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    total_records = len(dataset)
    print(f"Processing {total_records} floorplan samples into {output_dir}...")
    
    for index, record in enumerate(dataset):
        image = record.get(resolved_image_column)
        if image is None:
            print(f"Skipping record {index}: no image found.")
            continue

        pil_image = image.convert("RGB")
        mask, room_polygons, metadata, corner_points = extract_room_polygons(pil_image)
        rgb_image = np.array(pil_image)
        _, blended = colourise_rooms(rgb_image, room_polygons, corner_points)

        identifier = _normalise_identifier(_derive_record_identifier(record, index))
        floor_id = index + 1  # 1-based indexing for floor plans

        sample_dir = output_dir / f"floor{floor_id:03d}"
        sample_dir.mkdir(parents=True, exist_ok=True)

        mask_path = sample_dir / "room_mask.png"
        overlay_path = sample_dir / "overlay.png"
        json_path = sample_dir / "metadata.json"

        Image.fromarray(mask.astype(np.uint8)).save(mask_path)
        Image.fromarray(blended).save(overlay_path)

        # Extract supporting text if available and this record has text
        supporting_text = None
        if resolved_text_column and floor_id not in no_text_ids:
            text_value = record.get(resolved_text_column)
            if text_value is not None:
                if isinstance(text_value, str) and text_value.strip():
                    supporting_text = text_value.strip()
                elif isinstance(text_value, (list, tuple)) and len(text_value) > 0:
                    # Handle list of strings
                    text_str = " ".join(str(item).strip() for item in text_value if item)
                    if text_str.strip():
                        supporting_text = text_str.strip()

        payload = {
            "identifier": identifier,
            "record_index": index,
            "floor_id": floor_id,
            "floor_folder": sample_dir.name,
            "image_size": {"width": int(rgb_image.shape[1]), "height": int(rgb_image.shape[0])},
            "room_count": len(metadata),
            "rooms": _prepare_room_metadata(metadata),
            "corner_points": [list(point) for point in corner_points],
            "generated_files": {
                "room_mask_png": mask_path.name,
                "overlay_png": overlay_path.name,
                "metadata_json": json_path.name,
            },
        }
        
        # Add supporting_text at the top if available
        if supporting_text is not None:
            payload = {"supporting_text": supporting_text, **payload}

        with json_path.open("w", encoding="utf-8") as metadata_file:
            json.dump(payload, metadata_file, indent=2)

        if (index + 1) % 25 == 0 or index == total_records - 1:
            print(f"Processed {index + 1}/{total_records} samples")
    
    print()
    print("=" * 50)
    print("Processing complete!")
    print(f"Processed data saved to: {output_dir}")
    print("=" * 50)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Format FloorPlans970 dataset: download, load, extract masks, and save to processed."
    )
    parser.add_argument(
        "--dataset-path",
        type=Path,
        default=DEFAULT_LOCAL_DATASET_PATH,
        help="Path to a locally downloaded dataset saved via `datasets.save_to_disk`.",
    )
    parser.add_argument(
        "--dataset-name",
        type=str,
        default=DEFAULT_DATASET_NAME,
        help="Hugging Face dataset identifier to download if no local copy exists.",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="train",
        help="Split name to load when the dataset provides multiple splits.",
    )
    parser.add_argument(
        "--image-column",
        type=str,
        default=None,
        help="Override image column in the dataset.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=None,
        help="Directory to save processed floor plans (default: processed/floor_plans).",
    )
    parser.add_argument(
        "--force-download",
        action="store_true",
        help="Force re-download even if dataset exists locally.",
    )
    return parser.parse_args()


def main() -> None:
    """Main entry point for the data formatting workflow."""
    args = parse_args()
    process_dataset(
        dataset_path=args.dataset_path,
        dataset_name=args.dataset_name,
        split=args.split,
        image_column=args.image_column,
        output_dir=args.output_dir,
        force_download=args.force_download,
    )


if __name__ == "__main__":
    main()

